{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the required imports.\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import namedtuple\n",
    "import gensim\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Doc2Vec,Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>que_1_tok</th>\n",
       "      <th>que_2_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>['What', 'is', 'the', 'step', 'by', 'step', 'g...</td>\n",
       "      <td>['What', 'is', 'the', 'step', 'by', 'step', 'g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>['What', 'is', 'the', 'story', 'of', 'Kohinoor...</td>\n",
       "      <td>['What', 'would', 'happen', 'if', 'the', 'Indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>['How', 'can', 'I', 'increase', 'the', 'speed'...</td>\n",
       "      <td>['How', 'can', 'Internet', 'speed', 'be', 'inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>['Why', 'am', 'I', 'mentally', 'very', 'lonely...</td>\n",
       "      <td>['Find', 'the', 'remainder', 'when', '[', 'mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>['Which', 'one', 'dissolve', 'in', 'water', 'q...</td>\n",
       "      <td>['Which', 'fish', 'would', 'survive', 'in', 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                           que_1_tok  \\\n",
       "0  ['What', 'is', 'the', 'step', 'by', 'step', 'g...   \n",
       "1  ['What', 'is', 'the', 'story', 'of', 'Kohinoor...   \n",
       "2  ['How', 'can', 'I', 'increase', 'the', 'speed'...   \n",
       "3  ['Why', 'am', 'I', 'mentally', 'very', 'lonely...   \n",
       "4  ['Which', 'one', 'dissolve', 'in', 'water', 'q...   \n",
       "\n",
       "                                           que_2_tok  \n",
       "0  ['What', 'is', 'the', 'step', 'by', 'step', 'g...  \n",
       "1  ['What', 'would', 'happen', 'if', 'the', 'Indi...  \n",
       "2  ['How', 'can', 'Internet', 'speed', 'be', 'inc...  \n",
       "3  ['Find', 'the', 'remainder', 'when', '[', 'mat...  \n",
       "4  ['Which', 'fish', 'would', 'survive', 'in', 's...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset. Viewing its head\n",
    "train = pd.read_csv('qa_train_final.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, We have reduced the size of the datasets. And we can see that the allocation of duplicate/Not-duplicate sentences are not imbalanced. That is the reason why we are proceeding with this data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the tokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the tokenizing function on the dataset\n",
    "train['question1'] = train['question1'].apply(str)\n",
    "train['question2'] = train['question2'].apply(str)\n",
    "train['que_1_tok'] = train['question1'].apply(lambda x : tokenizer.tokenize(x))\n",
    "train['que_2_tok'] = train['question2'].apply(lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are generating three basic features from teh data at hand.\n",
    "# 1. Grtting the length of sentences for question 1\n",
    "# 2. Getting the length of sentences for question 2\n",
    "# 3. Getting the length difference\n",
    "train['q1_len'] = train['que_1_tok'].apply(lambda x: len(x))\n",
    "train['q2_len'] = train['que_2_tok'].apply(lambda x: len(x))\n",
    "train['len_diff'] = abs(train.q1_len - train.q2_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function for calculating the common words, and normalizing them with the length of the questions,.\n",
    "def normalized_common_words(df):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), df['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), df['question2'].split(\" \")))    \n",
    "    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "\n",
    "\n",
    "train['word_share'] = train.apply(normalized_common_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to remove the stopwords and removing any non-numeric words.\n",
    "def rem_stpwrds(x):\n",
    "    \n",
    "    x = [word for word in x if word not in stopwords.words('English')]\n",
    "    x = [word for word in x if word.isalpha() == True]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the remove_stopwords function.\n",
    "train['que_1_stp_wrds'] = train.que_1_tok.apply(lambda x: rem_stpwrds(x))\n",
    "train['que_2_stp_wrds'] = train.que_2_tok.apply(lambda x: rem_stpwrds(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are generating three basic features from teh data at hand.\n",
    "# 1. Grtting the length of sentences for question 1\n",
    "# 2. Getting the length of sentences for question 2\n",
    "# 3. Getting the length difference \n",
    "train['q1_stpwrds_len'] = train['que_1_stp_wrds'].apply(lambda x: len(x))\n",
    "train['q2_stpwrds_len'] = train['que_2_stp_wrds'].apply(lambda x: len(x))\n",
    "train['len_diff_stpwrds'] = abs(train.q1_stpwrds_len - train.q2_stpwrds_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any Null values\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to Check if the first word of the questions are same.\n",
    "def prefix_match(row):\n",
    "    \n",
    "    for i in range(len(train)):\n",
    "        \n",
    "        b1 = str(row['que_1_tok'][i][0])\n",
    "        b2 = str(row['que_2_tok'][i][0])    \n",
    "\n",
    "        if b1 == b2:\n",
    "            return 1\n",
    "    \n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "# Applying the function.\n",
    "train['prefix_match'] = train.apply(prefix_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to check if the last words are similar.\n",
    "def last_word_match(row):\n",
    "    \n",
    "    for i in range(len(train)):\n",
    "        \n",
    "        b1 = str(row['que_1_stp_wrds'][i][-1])\n",
    "        b2 = str(row['que_2_stp_wrds'][i][-1])    \n",
    "\n",
    "        if b1 == b2:\n",
    "            return 1\n",
    "    \n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "# Applying the function.\n",
    "train['last_word_match'] = train.apply(last_word_match, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a TF_IDF Model. Here are some of its attributes:\n",
    "# min_df = Removes the vocabulary which has its count less than 2 times accross all the questions.\n",
    "# analyzer = It takes a single word for creating the TF/IDF. 1-gram\n",
    "# token_pattern = It considers the string.\n",
    "# ngram_range = giving teh ngram range, so it uses uni and bi gram approach.\n",
    "# smooth_idf = this prevents the dividing by zero error. 1 - laplace smoothing.\n",
    "tf_idf_vecs = TfidfVectorizer(min_df=2,  max_features=None, strip_accents='unicode',  \n",
    "      analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), \n",
    "      use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "            \n",
    "# Fit TFIDF    \n",
    "tf_idf_vecs.fit(pd.concat([train['question1'],train['question2']]))\n",
    "\n",
    "# fit the Tf-IDF model on teh questions and transform the questions into vectors.\n",
    "tfidf_que1_vecs = tf_idf_vecs.transform(train['question1']) \n",
    "tfidf_que2_vecs = tf_idf_vecs.transform(train['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the shapes of output TF-IDF Vectors.\n",
    "print (tfidf_que1_vecs.shape)\n",
    "print (tfidf_que2_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Word2Vec Model for Word Embeddings of the data. Some of its attributes are:\n",
    "# size: It is the size of the output vector for each word.\n",
    "# alpha: It is the learning rate\n",
    "model_w2v = Word2Vec(list(train.que_1_stp_wrds) + list(train.que_2_stp_wrds),\n",
    "    size=300,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5) \n",
    "\n",
    "# Saving the model.\n",
    "model_w2v.save('w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generated Model\n",
    "model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the vector size of each word\n",
    "model_w2v.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the unique vectors in teh vocabulary\n",
    "len(model_w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most similar word for each word in teh questions.\n",
    "model_w2v.most_similar('What')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "model_gv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model_gv['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gv.most_similar('Selfie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pair questions from the dataframe\n",
    "a = train.que_1_stp_wrds[5]\n",
    "b = train.que_2_stp_wrds[5]\n",
    "print('que-1:',a)\n",
    "print('que-2:',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors for question 1 using the GoogleNews Model\n",
    "vector1 = model_gv[a]\n",
    "vector1 = vector1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors for question 2 using GoogleNews Model\n",
    "vector2 = model_gv[b]\n",
    "vector2 = vector2.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are trying to find out the vector similarity score using Cosine Distance.\n",
    "sp.spatial.distance.cosine(vector1,vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = d2v_model.infer_vector(a)\n",
    "v2 = d2v_model.infer_vector(b)\n",
    "sp.spatial.distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the questions and qids into a single dataset.\n",
    "train_set1 = train[[\"qid1\",\"question1\"]]\n",
    "train_set2 = train[[\"qid2\",\"question2\"]]\n",
    "\n",
    "train_set1.columns = [\"qid\",\"question\"]\n",
    "train_set2.columns =[\"qid\",\"question\"]\n",
    "train_set = pd.concat([train_set1,train_set2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the required lists for storing teh que id with the question tagged to it. The input style for the Doc2vec\n",
    "alldocuments = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')       \n",
    "keywords = []\n",
    "\n",
    "# Method to built a tagged document with qid as tag id and que as the sentence. \n",
    "# This model dosent consider teh stopwords.\n",
    "for index,record in train_set[:].iterrows():\n",
    "    qid = str(record[\"qid\"])\n",
    "    question = str(record[\"question\"])\n",
    "    question = tokenizer.tokenize(question)\n",
    "    #tokens = rem_stpwrds(question)\n",
    "    #print(tokens)\n",
    "    words_text = \" \".join(question)\n",
    "    words = gensim.utils.simple_preprocess(words_text)\n",
    "    tags = [qid]\n",
    "    alldocuments.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for Doc2vec model building. Max iterations is 100, learning rate is 0.025\n",
    "# the vector size for each sentence being 300.\n",
    "max_epochs = 100\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "# Here we are decreasing the learning rate linearly.\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                window=5, \n",
    "                seed=1337,\n",
    "                workers=4,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =0)\n",
    "\n",
    "# Building the Doc2vec model.\n",
    "model.build_vocab(alldocuments)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(alldocuments,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# Saving the model\n",
    "model.save(\"d2v_sentences.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to not build the model again and just trying to load the model.\n",
    "d2v_model = Doc2Vec.load('d2v_sentences.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the vector similarity of questions using Doc2Vec model. (Without Stopwords)\n",
    "def cos_dist_d2v_stpwrds(df):\n",
    "            \n",
    "    vec1 = d2v_model.infer_vector(df['que_1_stp_wrds'])\n",
    "    vec2 = d2v_model.infer_vector(df['que_2_stp_wrds'])\n",
    "        \n",
    "    cdist = sp.spatial.distance.cosine(vec1,vec2)\n",
    "        \n",
    "    return cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the vector similarity of questions using Doc2Vec model. (With Stopwords)\n",
    "def cos_dist_d2v(df):\n",
    "            \n",
    "    vec1 = d2v_model.infer_vector(df['que_1_tok'])\n",
    "    vec2 = d2v_model.infer_vector(df['que_2_tok'])\n",
    "            \n",
    "    cdist = sp.spatial.distance.cosine(vec1,vec2)\n",
    "        \n",
    "    return cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above methods to generate the cosine distances.\n",
    "train['cosine_dist_d2v_stpwrds'] = train.apply(cos_dist_d2v_stpwrds, axis=1)\n",
    "train['cosine_dist_d2v'] = train.apply(cos_dist_d2v, axis=1)\n",
    "train.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the data and splitting into train and test.\n",
    "X = tfidf_que1_vecs + tfidf_que2_vecs\n",
    "y = train['is_duplicate']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)\n",
    "\n",
    "# Building the log model and fitting.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the model Evaluation metrics.\n",
    "accuracy = accuracy_score(y_test,model_pred)\n",
    "precision = precision_score(y_test,model_pred)\n",
    "recall = recall_score(y_test,model_pred)\n",
    "print('The accuracy is: ',accuracy)\n",
    "print('The precision is: ',precision)\n",
    "print('The recall is: ',recall)\n",
    "print('\\n')\n",
    "print(classification_report(y_test,model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the data and splitting into ttrain and test.\n",
    "X = train[['q1_len', 'q2_len', 'len_diff', 'word_share','q1_stpwrds_len', 'q2_stpwrds_len',\n",
    "       'len_diff_stpwrds', 'prefix_match', 'last_word_match',\n",
    "       'cosine_dist_d2v_stpwrds', 'cosine_dist_d2v']]\n",
    "y = train['is_duplicate']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)\n",
    "\n",
    "# Building teh log model and fitting.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the model Evaluation metrics.\n",
    "accuracy = accuracy_score(y_test,model_pred)\n",
    "precision = precision_score(y_test,model_pred)\n",
    "recall = recall_score(y_test,model_pred)\n",
    "print('The accuracy is: ',accuracy)\n",
    "print('The precision is: ',precision)\n",
    "print('The recall is: ',recall)\n",
    "print('\\n')\n",
    "print(classification_report(y_test,model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#spam_detect_model = MultinomialNB().fit(, messages['label'])\n",
    "\n",
    "# Assigning the data and splitting into train and test.\n",
    "X = tfidf_que1_vecs + tfidf_que2_vecs\n",
    "y = train['is_duplicate']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)\n",
    "\n",
    "# Building the log model and fitting.\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the model Evaluation metrics.\n",
    "accuracy = accuracy_score(y_test,model_pred)\n",
    "precision = precision_score(y_test,model_pred)\n",
    "recall = recall_score(y_test,model_pred)\n",
    "print('The accuracy is: ',accuracy)\n",
    "print('The precision is: ',precision)\n",
    "print('The recall is: ',recall)\n",
    "print('\\n')\n",
    "print(classification_report(y_test,model_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
